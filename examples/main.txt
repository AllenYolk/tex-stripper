Results

Balancing expressivity and computational cost of dendritic spiking neuron

Distinguishing themselves from point neuron models widely employed in deep learning, multi-compartment models in computational neuroscience incorporate internal spatial structures and neuronal dynamics, as demonstrated in Figure 1b. These intricate models depict neuronal morphology at a micrometer level and portray nonlinear dynamics using an extensive set of variables  [1], hence being capable of capturing complex behavior patterns of biological neurons. Nevertheless, their computational complexity impedes their application in deep neural networks, even with optimal acceleration methods in place [2]. In response, (Note: this is a footnote!!) reasonable simplifications are imperative to simulate a large population of complex neurons while preserving essential structural and computational properties.

Figure 1: The DendSN model and its simulation results. a) A schematic illustration of a rat layer-5 pyramidal cell's (L5PC) morphology [3]. b) Our proposed DendSN model. It simplifies a multi-compartment model from three aspects: spatial structure, local dynamics, and signal transmission mechanism (blue boxes). A strength value is assigned to each dendritic branch at each time step, greatly enhancing the model's expressivity (red box). c), d) The computational overhead of different neuron models on a GPU. Here, ``DendSN, Parallel" means a DendSN whose dendritic dynamics are computed in parallel across both time and branches, while ``DendSN, Vanilla" is a DendSN simulated with the vanilla Euler method. ``Simulation time cost" refers to the total time required to sequentially process 512 randomly generated input samples of length <MATH>. The data in Subfigure d is obtained by averaging the time costs along all the conditions of <MATH> while keeping the <MATH> conditions. e) An illustration of the L5PC fitting experiment. For DendSN conditions, <MATH> is set to <MATH>, representing the apical and basal dendrites. The presynaptic spikes are weighted by synaptic strengths, summed, and then fed into the branches. Parameters in the circles are optimized using gradient descent. For point SN conditions, the weighted sum of all the spikes is directly fed into the soma. f) The fitting losses of different models on the test set. Here, ``USwish2" means USwish dendritic nonlinear activation (see <1>) and <MATH>. g),h) The somatic membrane potential of a detailed multi-compartment L5PC model (black, ground truth), a point neuron model (blue), and a DendSN with a fully learnable branch strength matrix (orange, dashed). The trial is selected from the test set.

Let's input the Introduction Section from main.tex:

Introduction

Figure 2: A comparison between deep neural networks with point neurons and biophysical neural network models with multi-compartment neurons. a) Typically, deep neural networks (lower) adopt point neuron models (upper) as their basic computational units. Both the perceptron in deep ANNs and the LIF model in deep SNNs belong to this category. They simplify the neuron's morphology to a single spatial point. b) Multi-compartment models (upper) in neuroscience capture the detailed morphologies of real neurons and have complex dendritic dynamics. By connecting these intricate neurons, biophysical network models (lower) are established to simulate small-scale neural circuits in the brain. Notice that there may be multiple synaptic connections between a pair of neurons. c) Deep neural networks based on point neurons are computationally efficient and convenient to construct and train, but fall short on bio-plausibility and single-neuron expressivity (blue box). Biophysical networks with multi-compartment neurons have remarkable expressivity and can simulate the complex properties of real neurons, but cannot be easily scaled up to large networks (green box). There is a gap between these two types of models. In this work, we propose DendSN and DendSNN, combining dendritic computation with the design principles of deep learning to power up deep SNNs. We mainly focus on two questions: how to ensure model flexibility and scalability while improving expressivity, and what the effects of dendritic computation on deep neural networks are in diverse machine learning scenarios.

Figure 3: A comparison between deep neural networks with point neurons and biophysical neural network models with multi-compartment neurons. a) Typically, deep neural networks (lower) adopt point neuron models (upper) as their basic computational units. Both the perceptron in deep ANNs and the LIF model in deep SNNs belong to this category. They simplify the neuron's morphology to a single spatial point. b) Multi-compartment models (upper) in neuroscience capture the detailed morphologies of real neurons and have complex dendritic dynamics. By connecting these intricate neurons, biophysical network models (lower) are established to simulate small-scale neural circuits in the brain. Notice that there may be multiple synaptic connections between a pair of neurons. c) Deep neural networks based on point neurons are computationally efficient and convenient to construct and train, but fall short on bio-plausibility and single-neuron expressivity (blue box). Biophysical networks with multi-compartment neurons have remarkable expressivity and can simulate the complex properties of real neurons, but cannot be easily scaled up to large networks (green box). There is a gap between these two types of models. In this work, we propose DendSN and DendSNN, combining dendritic computation with the design principles of deep learning to power up deep SNNs. We mainly focus on two questions: how to ensure model flexibility and scalability while improving expressivity, and what the effects of dendritic computation on deep neural networks are in diverse machine learning scenarios.

This is a subsubsection title!!!

The past decade has witnessed the success of deep learning across diverse domains, including computer vision [4], natural language processing [5], and autonomous driving [6]. With the surge in the performance of parallel computing devices like GPUs, increasingly deeper artificial neural networks (ANNs) with meticulously crafted architectures can be efficiently trained and conveniently deployed for real-world applications [7]. In addition, inspired by the information processing mechanisms of biological neural circuits, spiking neural networks (SNNs) have emerged as a potentially more bio-plausible and energy-efficient alternative to ANNs [8]. Recent advances in neuromorphic hardware [9] and SNN programming frameworks [10] have further fueled the interest in deep SNNs, positioning them as promising models for the next generation of neural networks [11].

The fundamental issue in modern SNN researches is to combine spiking neuronal dynamics with deep network architectures and further identify the most suitable application scenarios. Cutting-edge studies in this field are leveraging ANN building blocks like residual connections [12] and self-attention mechanisms [13] for building up their spiking variants [14], powering up deep SNNs and yielding enhanced performance. However, much less attention is directed towards the intricacies of the underlying neuron model. Previous studies in neuroscience have found that the computational expressivity of a single biological neuron rivals that of a multi-layer neural network consisting of thousands of artificial neurons [15]. This suggests an urgent need to rethink the design of individual neurons in deep neural networks, and biological neurons may serve as a valuable source of inspiration for possible enhancements. Nevertheless, how to integrate the remarkable expressivity of biological neurons into neurons of deep neural networks for improved performance in practical tasks remains to be explored.

The dynamic mechanisms of biological neurons arise mainly from two sources: dendritic computations and somatic dynamics. Most existing deep SNNs, however, are built upon oversimplified neurons such as the leaky integrate-and-fire (LIF) model [16], treating the entire neuron as a single spatial point and only depicting the dynamic properties of the soma. Models like the parametric leaky integrate-and-fire (PLIF) neuron [17] and the few-spikes neuron (FS-neuron) [18] attempt to enhance neuron-level dynamics by introducing learnable internal parameters, but still fail to take dendrites into account. These spiking neuron models, as well as their non-spiking counterparts in ANNs [19], are termed point neuron models due to their reduced morphology and dynamics (Figure 2a). Notwithstanding their computational efficiency and reduced memory usage, these prevalent models sacrifice essential computational properties of dendrites, resulting in limited single-neuron expressivity. The oversimplification of point neuron models is a bottleneck that constrains the overall expressivity of deep SNNs and hinders the networks' ability to perform intricate computations for challenging tasks, as summarized within the blue box in Figure 2c.

Let's include a table from Introductin:

Table 1: You know, this is a table!!! Yes! Look at me!

In stark contrast, neuroscientists use interconnected multi-compartment models to simulate the biophysical activities of small-scale neuron circuits in the brain [20]. As illustrated in Figure 4b, multi-compartment models portray neuronal morphology at a fine granularity, and use differential equations with hundreds of variables to describe neuronal dynamics [21]. They manage to capture critical computational properties present in biological dendrites [22], including the passive attenuation of input signals [23], the active generation of dendritic spikes [24], as well as the selection and multiplexing of information [25]. As a result, multi-compartment models exhibit remarkable neuron-level expressivity (Figure 4c, green box). Adding structural complexity and dendritic nonlinear dynamics to the neurons in deep SNNs is thus an intuitive and promising strategy for elevating the networks' expressivity and performance. The research community, however, has overlooked the role of dendritic computation in deep SNNs so far.

The idea of incorporating dendritic computation into deep learning, nonetheless, poses challenges related to network scalability and model flexibility. The success of deep learning models is primarily attributed to their large scales and intricate architectures. However, simulating large populations of multi-compartment neurons with complex dynamics is computationally prohibitive; even if these models can be simplified to reduce computational overhead, they are not trivially compatible with today's complex network architectures (see Figure 6c, green box). For instance, some pioneering works focusing on bio-plausibility encounter scalability difficulties as network sizes increase due to the burden of solving differential equations for complicated neuronal dynamics [26]. A recent study by Zheng et al. [27] simplifies multi-compartment models and proposes the DH-LIF neuron with temporal dendritic heterogeneity. They manage to build fully connected dendritic SNNs with recurrent connections and achieve competitive performances on tasks with rich timescales. Nevertheless, their neuron model has not been applied to networks with more than five layers due to high computational overhead, nor has it extended to prevalent SNN architectures like convolutional networks and Transformers. Some ANN neurons incorporate properties of dendritic computation [28] while neglecting temporal dynamics. Yet, they remain confined to small-scale fully connected networks. Thus, the fundamental question of how to ensure model flexibility and scalability while improving neuron-level expressivity is still unresolved (Figure 6c, upper part of the ring). We emphasize the necessity of adapting dendritic neurons to diverse network architectures; we also claim that two core requirements must be met to overcome the scalability issues: (1) developing a dendritic neuron model with highly parallelized simulation algorithms, and (2) leveraging the computational power of modern GPUs through low-level programming languages. Tackling these challenges is crucial and imperative for the practical applications of dendritic neural networks to complicated tasks.

Furthermore, while dendritic computation has been extensively studied [29], its influences on deep neural networks remain largely unexplored. Most previous works narrowly focus on one aspect of dendrites' benefits to deep learning, lacking a comprehensive study on the diverse network-level impact of dendrites in various machine learning scenarios [30]. As demonstrated in the middle of Figure 8c, understanding the influence of dendritic computation on deep neural networks in diverse task settings is another critical issue that needs to be resolved.

To address these challenges, we propose the dendritic spiking neuron (DendSN) model that incorporates multiple dendritic branches with nonlinear dynamics. We demonstrate that DendSN has significantly higher expressivity than point spiking neurons, mainly due to its enhanced dendritic branch weighting mechanism. An acceleration algorithm is further proposed to parallelize dendritic dynamics computation over time, making the extra overhead negligible compared to point spiking neurons. We then introduce a universal solution to seamlessly integrate DendSNs into various deep SNN architectures, showcasing the flexibility of our model. Triton kernels [31] are developed to fully exploit the power of GPUs, enabling dendritic spiking neural networks (DendSNNs) to reach the same depths as traditional deep SNN architectures while keeping affordable computational costs. DendSNNs are compatible with existing large-scale SNN training methods, and demonstrate improved accuracies compared to point SNNs in static image and event-based data classification tasks, even without introducing extra learnable parameters. We next comprehensively evaluate DendSNNs' performances across various settings to show their superior network-level expressivity and robustness compared to point SNNs. Inspired by dendritic modulation mechanism and synaptic clustering phenomenon observed in biological neural circuits [32], we propose dendritic branch gating (DBG), a novel algorithm designed for continual learning. By adjusting dendritic branch strengths based on task context signals, DBG effectively reduces interference between synaptic inputs from different tasks. Moreover, additional experiments reveal that DendSN can enhance the robustness of SNNs against noise and adversarial attacks, with improved performances also observed in few-shot learning scenarios. To the best of our knowledge, this is the first work that demonstrates the possibility and feasibility of constructing and training SNNs incorporating multiple nonlinear dendritic branches with depth and scale comparable to traditional deep SNNs, and conducts a systematic study on the benefits of dendritic computation for deep networks in diverse machine learning scenarios.

Let's include a table from main.tex:

Table 1: You know, this is a table!!! Yes! Look at me!

We propose the dendritic spiking neuron (DendSN) model illustrated in Figure 9b to strike this delicate balance. The simulation process is carried out over <MATH> discrete time steps using the Euler method, just as the standard practice in SNNs. Structurally, the neuron comprises <MATH> parallel dendritic branches and a soma. The <MATH>-th dendritic branch possesses a single state variable <MATH>, representing the dendritic local potential. At each time step <MATH>, the potential undergoes an update based on synaptic input <MATH>, and decays exponentially over time with a factor <MATH>.

<MATH>

<MATH> is then activated by a nonlinear function <MATH>  and subsequently weighted by a factor <MATH>, yielding the branch-to-soma signal <MATH>. The total input to the soma is the sum of the signals from all the branches.

<MATH>

Finally, the soma is a LIF neuron with a unary state variable <MATH> as the somatic membrane potential [33]. It charges through the aggregated dendrite-to-soma signal <MATH>, decays exponentially with a factor <MATH>, emits a spike when <MATH> exceeds a threshold <MATH>, and resets to <MATH> after firing. The somatic dynamics can be described as

<MATH>

where <MATH> is the somatic potential before spike emission, and <MATH> is the Heaviside step function.

Table 1: You know, this is a table!!! Yes! Look at me!

Compared to detailed multi-compartment neurons, DendSN makes simplifications on spatial structure, source of nonlinearity, and dendritic local dynamics to reduce its computational complexity (see the blue boxes in Figure 10b). Firstly, the dendritic tree is portrayed as <MATH> electrically segregated branches, each modeled as a single compartment. This abstraction is a nod to the common presence of multiple dendritic branches in biological neurons like pyramidal cells [34] (Figure 10a) while intentionally forgoing intricate morphological details. It not only significantly reduces memory costs for representing neuronal morphology, but also eliminates the need for transmitting signals between dendritic compartments, improving simulation efficiency. Secondly, nonlinear branch activation functions <MATH> are introduced to directly map dendritic states to branches' overall contributions to somatic potential change (see Supplementary Information 12 for more details). Dendritic nonlinearity, one of the key computational properties of dendritic computation, is attached to the model in a general manner by these element-wise functions. Hence, we don't have to explicitly model the complex biophysical processes of gated dendritic ion channels using additional state variables and simulate their dynamics at high temporal resolution. Thirdly, since all the nonlinear components are shifted to the dendrite-to-soma signal forwarding process, the local dynamics of each dendritic state <MATH> remain completely linear, merely reflecting passive electrical property [35]. Thanks to their linearity, dendritic states can be updated in parallel across different branches as well as time steps. The acceleration algorithm of dendritic state computation is further explained by Equation (1) and (2) in <2>.

Under these simplifications, DendSN can be simulated with high efficiency on parallel computing devices like GPUs. As shown in Figure 13c, the simulation time cost of DendSN remains almost constant as the number of dendritic branches <MATH> increases, whether the acceleration of dendritic state update is applied or not. We attribute the constancy to the parallel arrangement of dendritic compartments, which allows for simultaneous updates of all the B dendritic states. Detailed multi-compartment models with <MATH> dendritic compartments, by contrast, inevitably suffer from a noticeable increase in simulation time cost as <MATH> grows, since the state of a particular compartment depends on the states of all its parent nodes [36]. Apart from that, Figure 13d suggests that the simulation time costs of both point neuron and DendSN grow linearly with the number of time steps <MATH>. If the parallelism of dendritic state update across time is exploited, DendSN's simulation efficiency can be boosted to a level almost identical to that of the point neuron. Thus, the extra computational load imposed by dendrites is negligible in practice. These observations indicate that DendSN is computationally more affordable than conventional multi-compartment models, and holds promise for application in deep neural networks.

Despite its relatively low computational cost, DendSN retains a remarkable expressivity, mainly due to the enhancement of the branch weighting mechanism (see the red box in Figure 15b). The branch strength matrix <MATH> assigns weights to different branches at different time steps, having the capacity to model the complex dependency of the somatic potential on dendritic states. To show that <MATH> is the key factor endowing DendSN with the ability to capture crucial computational properties of biological neurons, we conduct a fitting experiment using activity data from a multi-compartment biophysical model of a layer-5 pyramidal cell [37], as illustrated in Figure 15e. By leveraging gradient descent to learn both the synaptic weights and <MATH> (if DendSN is used), a single reduced neuron model can approximate the mapping from presynaptic spikes to somatic membrane potential of the detailed model. Figure 15f demonstrates that DendSNs with fully learnable <MATH> can achieve significantly better performances than a point LIF model, regardless of the dendritic activation function used. DendSN with non-monotonic Mexican hat dendritic nonlinearity yields the lowest mean squared error (MSE) on the test set, about <MATH> of LIF neuron's MSE. However, if <MATH>'s degree of freedom is restricted by setting it as a constant matrix or enforcing its rows or columns to be identical, the approximation and generalization performance of DendSN will degrade to LIF's level. To be worse, models with Mexican hat dendritic activation function may fail to converge on the training set under these constraints. In Figure 15g, we visualize the potential sequences of the original biophysical model (black), a LIF neuron (blue), and a DendSN with fully learnable <MATH> and Mexican hat activation (orange, dashed), given the same input spike train. Notably, DendSN approximates the ground truth with high fidelity when the somatic potential is far below the firing threshold, while the potential of the point neuron fluctuates severely and deviates from the ground truth. The smoothness of DendSN's potential trace is a direct result of its passive dendritic dynamics, which proves to be beneficial in this task. Although DendSN fails to predict the potential when the ground truth is close to the firing threshold (see the upper right box in Figure 15g, as well as Figure 15h), the decent performance in the subthreshold regime indicates DendSN's solid biological plausibility and marvelous expressivity.

In summary, DendSN stands as a streamlined yet expressive model for dendritic neurons. The branch weighting mechanism enhances the neuron's capacity, while the simplifications of dendritic morphology and nonlinear dynamics enable parallel computation across branches and time, reducing computational overhead. Its excellent expressivity and efficiency make it suitable for deep learning applications.

Dendritic spiking neural networks and their supervised training

Figure 4: Dendritic spiking neural networks and their supervised learning performance. a),b) Detailed illustrations showcasing how input features <MATH> are assigned to different dendritic branches of a DendSN layer following either a fully connected layer (Subfigure a) of a 2D convolutional layer (Subfigure b). c) A comparison between a fully connected point SNN and a DendSNN with similar architecture. By replacing the point neuron layers with DendSN layers and adjusting the number of channels, a point SNN architecture can be easily converted to a DendSNN. d) Classification accuracies on the FashionMNIST benchmark. DendSNNs consistently outperform point SNNs, no matter which dendritic activation functions are used, how many dendritic branches are employed, and how branch strength matrices are set. e) A sample from the event-based N-MNIST dataset. d) Classification accuracies on the N-MNIST dataset. DendSNNs achieve better performances than point SNNs on different time resolution settings (<MATH>).

The efficacy of modern deep learning stems from the vast scale and well-crafted structure of neural networks. Deep network architectures like ResNet [38] and Transformer [39] have been widely embraced for tackling machine learning problems in diverse domains. Pioneering works in the field of SNN have leveraged these advantageous design principles and built up their spiking counterparts, outperforming those plain and shallow SNNs [40]. Here, we integrate our DendSN into various deep SNNs with commonly used architectures, aiming to fully harness both the expressive power of DendSN and the capabilities of deep networks for tackling complex tasks.

To embed DendSNs into deep SNNs, an ensemble of neurons is first organized as a layer to facilitate tensor-based parallel simulation. For simplicity, we assume that all DendSNs in the layer share the same neuronal parameters <MATH>, except for the branch strength matrix <MATH>. By default, all the neuronal parameters stay constant during both training and inference to avoid introducing extra learnable weights. The DendSN layer is positioned after a weight layer, forming a weight-DendSN block. The weight layer acts as synaptic connections; unlike previous works on dendritic deep learning, we do not impose an explicit sparsity constraint on the weight layer [41] for the sake of simplicity and computational efficiency (see 21 for more details). The output of the weight layer is then folded along the channel (feature map) dimension into <MATH> folds, where the <MATH>-th fold serves as the input to the <MATH>-th branch of all DendSNs in this layer.

<MATH> Here, <MATH> denotes the <MATH>-th feature map of the weight layer's output, and <MATH> represents the input to the <MATH>-th dendritic branch of all DendSNs at channel <MATH> in this layer (refer to <3> for detailed explanations). Consequently, a DendSN layer reduces the number of feature maps to <MATH> of its original value, while retaining the size of the spatial dimensions. Note, Equation (3) provides a universal solution to the input feature assignment problem for DendSN layers after different types of weight layers. Figure 22a and Figure 22b demonstrate two specific cases for fully connected and 2D convolutional layers, respectively. Supplementary Information 24 provides PyTorch implementations of a DendSN layer, either accelerated using temporal parallelism mentioned in the last section or not.

By cascading multiple weight-DendSN blocks, a deep DendSNN can be established. In fact, nearly all prevalent SNN designs can be converted to their DendSNN counterparts, thanks to the flexibility of the DendSN layer design as described in Equation (4). From the perspective of deep learning, DendSN layers serve as nonlinear activation functions, akin to the role point spiking neuron layers play in traditional SNNs. The main difference lies in the <MATH> feature compression introduced by a layer of <MATH>-branch DendSNs. Therefore, a DendSNN architecture can be intuitively built up by first replacing the point spiking neuron layers in a conventional SNN with DendSN layers, and then adjusting the number of feature maps in the weight layers to align with the requirements of DendSN layers, as Figure 25c suggests. This process can be implemented by popular deep learning frameworks like PyTorch [42] in a plug-and-play manner, making DendSNNs highly accessible and flexible for deep learning practitioners.

In addition to flexibility, computational efficiency is a crucial requirement for deep DendSNNs. As illustrated in Figure 26d from the previous section, DendSN achieves low simulation cost by enabling parallel dendritic state updates both across branches and over time. This parallelism forms the algorithmic foundation for the efficient computation of deep DendSNNs. We also employ hand-crafted Triton kernels [43] to leverage the full power of GPUs. This combination of algorithmic improvement and GPU-level acceleration allows DendSNNs to be trained and evaluated with high efficiency, ensuring their scalability for large-scale tasks.

The dynamics of a DendSN layer are fully differentiable, except for the Heaviside step function in the spike generation process. This issue can be circumvented using surrogate gradient [44], a common practice in directly trained point SNNs. Hence, DendSNNs can be trained in a supervised manner using gradient-based optimization methods like spatio-temporal backpropagation (STBP, a special case of BPTT) [45], enabling them to tackle a wide range of complex tasks. Here, we use two toy datasets to show the feasibility of training DendSNNs; in later sections, we will train DendSNNs with depths and scales comparable to prevalent deep point SNNs to solve more challenging tasks. By optimizing synaptic weights, 3-layer fully connected DendSNNs can achieve persistently better classification accuracies on the FashionMNIST [46] dataset, compared to a point SNN with a similar architecture and equal number of learnable parameters, no matter which dendritic activation function and how may dendritic branches are employed (Figure 27d). Moreover, the expressivity of DendSNNs can be further augmented by appropriately setting the branch strength matrices <MATH> for each DendSN in the network. We introduce three advanced models for setting up the strength matrices: direct learning, CBAM-based multi-dimensional attention on dendrite-to-soma signal (MA) [47], and SimAM-inspired parameter-free attention (PFA) [48]. Experiments on FashionMNIST indicate that PFA marginally improves the classification accuracy of DendSNN with 5 branches and Mexican hat dendritic activation (see Figure 27d) even though no extra learnable parameters are brought in. The other two methods, in contrast, slightly degrade the model's generalization performance. Further experiments are conducted on the event-based N-MNIST [49] dataset (Figure 27e) using a convolutional network structure, under different temporal resolutions. Figure 27f reveals a trend similar to that of FashionMNIST, except for the fact that learning <MATH> directly improves models' performance whereas PFA does not. For more information about the results of the experiments, refer to Table 31 and 32. Notice that DendSNNs with MA cannot converge on the N-MNIST training set, so it is omitted from the figure. Our experience suggests that MA works on small fully connected DendSNNs with large <MATH>, while larger convolutional DendSNNs with MA mechanism and a small <MATH> value are hard to train with MA at the beginning.

All in all, the flexibility and efficiency of DendSN make the construction of large-scale DendSNNs possible. DendSNNs can achieve better performance on supervised learning tasks than point SNNs, even without introducing extra learnable parameters. The superiority of DendSNNs on ordinary supervised learning tasks is the basis for their application in more challenging scenarios.

Dendritic modulation for continual learning

Having presented our solution to incorporate dendritic computation into deep networks and the fact that DendSNNs exhibit remarkable expressivity, we now attempt to explore dendritic computation's influence on deep neural networks in demanding machine learning settings. Our initial focus is on continual learning, where a model should sequentially adapt to multiple data distributions without forgetting previously learned tasks. This ability is inherent in biological intelligence, reflecting the resilience of nervous systems. However, it poses a major challenge for artificial intelligent systems [50]. We argue that catastrophic forgetting of previously acquired knowledge can be alleviated by leveraging dendritic computation in a bio-inspired manner.

In the brain, sensory neurons receive not only bottom-up feedforward inputs but also top-down modulatory signals that adjust neuronal activity patterns [51]. These modulation inputs, typically originating from motor and prefrontal areas, encode higher-level information such as task context [52], thus bringing task-specific properties to sensory processing. Wybo et al. [53] revealed that NMDA-driven dendritic spikes may underlie contextual modulation of hierarchical sensory pathways, directing our attention to the role of dendritic modulation in task incremental learning.

What's more, the spatial location of synaptic sites on the dendritic tree greatly influences a biological neuron's response to synaptic inputs [54]. A pertinent hypothesis is synaptic clustering, which suggests that functionally related synapses tend to cluster on dendrites as the result of structural plasticity, as shown in Figure 33a. Previous studies have revealed that synaptic clusters are vital for increasing the brain's memory storage capacity [55], as functionally related inputs are preprocessed locally at the dendritic tree before integration at the soma, reducing interference from less relevant inputs. This hypothesis is supported by a wide range of anatomical and computational evidence [56].

Inspired by dendritic modulation and synaptic clustering, we propose a novel algorithm named dendritic branch gating (DBG) to mitigate catastrophic forgetting of DendSNNs in task-incremental learning scenarios (see Figure 34b and Supplementary Information 35). The index of the current task, denoted as <MATH>, serves as a top-down modulation signal and is fed to the network alongside the feedforward input during both training and inference. Note that the use of an extra context signal is a common practice in previously proposed architecture-based continual learning algorithms like XdG [57]. However, instead of modulating synaptic weights as done in XdG, our algorithm sets the branch strength matrices of all DendSNs in the network based on <MATH>. Mathematically, the modulation process can be described as

<MATH>

where <MATH> denotes the branch strength matrix of the <MATH>-th DendSN in the network, and <MATH> is the total number of DendSNs. The remaining issue lies in mapping <MATH> to a set of <MATH> matrices with shape <MATH>. The first approach, named <MATH>, assigns a set of dense but learnable matrices for each <MATH>. However, the density of <MATH> deviates from the principle of synaptic clustering, which implies that the synaptic weights learned from a particular task should cluster on a small subset of dendritic branches. To achieve this goal, the <MATH> mapping should yield sparse and temporally homogeneous (i.e. all <MATH> rows being identical) matrices. In response, we propose two more strategies: <MATH> and <MATH>, both of which satisfy these requirements. For sparse DBG implementations like these, training-time modulation prevents the learning of a new task from updating all the weights learned on previous tasks. Inference-time gating, on the other hand, ensures the activation of the correct subnetwork for each task. These properties are crucial for successfully avoiding catastrophic forgetting. Refer to <4> for a detailed explanation of DBG and its implementation, and Supplementary Information 36 for pseudocodes.

Figure 5: Dendritic branch gating (DBG) for mitigating catastrophic forgetting in task-incremental learning. a) An illustration of synaptic clustering in biological neurons. With the help of structural plasticity, functionally related synapses tend to cluster on dendrites, reducing interference from less relevant inputs. b) The DBG algorithm. During training and inference on task <MATH>, each DendSN in the network has its branch strength matrix <MATH> set as a matrix that depends on the context signal <MATH>. This work proposes three implementations of DBG: <MATH>, <MATH>, and <MATH>. The first two implementations set <MATH> as a sparse and temporally symmetric matrix, aligning with the synaptic clustering hypothesis. c) The Permuted MNIST tasks. In task <MATH>, all the samples are permuted using <MATH>. d),e),f) Results on the Permuted MNIST tasks. Subfigure e and f focus on DendSNNs with Mexican hat activation function to highlight the effect of DBG. g) The Split CIFAR-100 tasks. h),i) Results on the Split CIFAR-100 and Split TinyImageNet tasks.

To assess the task-incremental learning performance across different model types and modulation methods, we first use fully connected networks to solve the Permuted MNIST problem [58] consisting of <MATH> tasks (Figure 37c). Figure 37d depicts the average accuracy curves concerning the number of learned tasks, where the descending trends reflect the impact of catastrophic forgetting on these models. Surprisingly, DendSNNs demonstrate greater resilience to catastrophic forgetting than point SNN, particularly in the early stages of incremental learning (i.e. less than 20 learned tasks), even without the assistance of DBG. As the number of learned tasks increases, DendSNNs with leaky ReLU and unlearnable Swish [59] activation functions consistently outperform point SNN, while that with non-monotonic Mexican hat activation exhibits inferior performance. Next, we focus on the Mexican hat activation function cases, since the effect of DBG is more pronounced under this setting. Figure 37f showcases the final average accuracies on 50 tasks. After <MATH> is applied, DendSNNs achieve significantly higher average accuracy compared to the baseline point SNN. The overall performance improves with an increase in the number of dendritic branches <MATH>, as fewer tasks are allocated to each branch as <MATH> increases; the performance of each task, however, does not dramatically degrade with the decrease in size of the subnetwork corresponding to the task, which is consistent with the observation in a previous study [60]. When <MATH> is set to <MATH> and <MATH> is adopted, the average accuracy even surpasses the best result point-SNN-based XdG [61] can achieve. Figure 37e plots the heat map of task <MATH> accuracy after learning task <MATH> to <MATH> (<MATH> for the row index and <MATH> for the column index), indicating that the test accuracies of previous tasks do not degrade after learning new tasks with <MATH> applied to a DendSNN with <MATH> branches. If we use <MATH> on a 5-branch DendSNN, the test accuracies of previous tasks do degrade, as each branch cannot afford to learn 10 tasks. <MATH>, on the other hand, enhances the models' continual learning performance, and even outperforms <MATH> with <MATH>, as Figure 37e and Figure 37f show. Their average accuracies are comparable to those of XdG on point SNNs. Conversely, if <MATH> is applied, no significant improvement in average accuracy is observed compared with point SNN. Indeed, the performance is even worse than that of the baseline point SNN when <MATH>. This underscores the importance of employing sparse gating in DBG and promotes synaptic clustering for achieving biological-inspired continual learning. For more information about the results, see Table 43.

We conduct further experiments on the Split CIFAR-100 [62] and Split TinyImageNet datasets (details are provided in <5>), where deeper convolutional networks (SEW ResNets [63]) are trained. We utilize DendSNs with leaky ReLU dendritic activation instead of Mexican hat nonlinearity, since the latter one cannot scale up to deeper networks (see Supplementary Information 44). The task setting of Split CIFAR-100 is outlined in Figure 45g, with Split TinyImageNet following a similar structure. Figure 45h and Figure 45i show the average accuracies on the 10 Split CIFAR-100 tasks and 20 Split TinyImageNet tasks respectively. We observe similar trends to those observed in Permuted MNIST, with one notable exception: both vanilla DendSNNs with a small <MATH> value and DendSNNs with <MATH> outperform vanilla point SNNs. In addition, XdG yields intriguingly inferior performances on Split CIFAR-100. This phenomenon can be attributed to the fact that XdG zeros out most neuronal outputs, reducing the actual hidden size of each subnetwork and thereby impairing point SNNs' capacity. Sparse DBG methods, nevertheless, do not significantly hurt DendSNNs' capacity. For detailed experimental methodologies, refer to <5>. It is worth noting that DBG can be applied to DendSNNs together with powerful replay-based [64], regularization-based [65], and optimization-based [66] methods widely used in point neural networks to boost up the continual learning performance to an even higher level.

Apart from continual learning, DBG can be easily extended to multitask learning settings. The results of Multi-(Fashion+MNIST) [67] and UTKFace [68] multitask learning benchmarks are provided in Supplementary Information 48, where DendSNNs with some DBG conditions can obtain marginally better performance than point SNNs. In a nutshell, DBG provides a unified bio-inspired solution for task-context-aware learning, leading the deep learning community to exploit the inner structure of neurons for lifelong learning and multitask learning.

Performances on diverse machine learning scenarios

The exceptional task-context-aware learning performance of DendSNNs is propelled by a manually designed, although bio-inspired, algorithm. Additionally, we highlight the intrinsic advantages of the DendSN model for deep SNNs in this section.

Traditional ANNs and SNNs often falter when confronted with corrupted data, such as noisy inputs and adversarial attacks. They also encounter difficulties in few-shot learning scenarios, where training data are extremely scarce. By contrast, the human brain can handle all these challenges with ease, showcasing the remarkable robustness and generalization ability of biological neural circuits. We attribute this performance gap between artificial and biological systems to the disparities between real neural circuits and man-made models. To bridge this gap, a possible approach is to make artificial models more ``brain-like". We contend that the integration of dendritic computing into neural networks using DendSN has the potential to enhance performance and robustness in these challenging scenarios, even in the absence of specialized learning algorithms. We verify this hypothesis by conducting experiments on these scenarios, as listed in Figure 49a. We compare the performances of models using point SNNs or DendSNNs as the whole model or merely the backbone.

Figure 6: DendSNNs' noise robustness. a) In this section, we are going to test the performance of DendSNNs on three challenging machine learning scenarios: noise robustness, robustness against adversarial attacks, and few-shot learning. b) Samples from the FashionMNIST dataset with different levels of Gaussian noise infused (FashionMNIST-noise). c) DendSNNs' performances on FashionMNIST-noise. In the left plot, the model is trained on the clean training set and then tested on test sets with various noise levels. DendSNNs consistently outperform point SNNs. In the right plot, the model is trained on the noisy training set instead. Networks of DendSNs with 2 branches achieve the best performances. d) An illustration of the TinyImageNet-C dataset. Corruptions are added to each sample in the validation set. There are 15 corruption types, each with 5 levels of severity. e), f) DendSNNs' robustness on the CIFAR-10-C and TinyImageNet-C dataset. Their performances are compared with a point SNN (the pink dashed line), and mCE as well as rmCE are reported [69]. Lower mCE and rmCE are better. All the DendSNNs exhibit better robustness than the point SNN baseline on CIFAR-10-C, while on TinyIamgeNet-C, networks of DendSNs with 4 branches achieve significantly lower rmCEs compared to point SNN. g) An illustration of the DVS Gesture-C dataset. Six types of corruptions, each with 5 severity levels, are applied to each sample in the test set. h) DendSNNs' robustness on the DVS Gesture-C dataset. The baseline (the pink dashed line) is a point SNN. Five of the six DendSNNs achieve far lower rmCEs than point SNN.

Noise robustness

Noise robustness is crucial for deep learning models to maintain stable and reliable performance in real-world situations where data are susceptible to diverse sources of noise or corruption. To assess DendSNNs' robustness against noisy input, we conduct classification experiments on the FashionMNIST dataset [70] with varying levels of Gaussian noise infused (see Figure 50b). The models are first trained on either clean or noisy training sets, and then evaluated on the test sets with different noise levels. As shown in Figure 50c, with the increase in noise level, the classification accuracies of all the models decrease. Nonetheless, when trained on the clean training set, DendSNNs consistently outperform point SNNs across all noise levels, showing better noise robustness. Even when trained with noisy data, networks of 2-branch DendSNs still achieve the best performances, exhibiting slower accuracy decrease rates than point SNNs. These findings suggest that multi-branch dendritic structures are beneficial for SNNs to resist noise.

However, Gaussian noise alone cannot cover the diverse corruption types in real life. To this end, we comprehensively test DendSNNs' robustness against various noise types on CIFAR-10-C and TinyImageNet-C [71]. The networks are first trained on the clean training set and frozen after that. Then, their error rates are obtained on the corrupted validation set, which comprises multiple corruption types (20 for CIFAR-10-C and 15 for TinyIamgeNet-C), each with 5 levels of severity (see Figure 52d). We aggregate the models' error rates across corruption types and severities using the mean corruption error (mCE) and relative mean corruption error (rmCE) metrics introduced by Hendrycks et al. [71] (see <7>), and report the results in Figure 52e and Figure 52f. On the CIFAR-10-C task, all DendSNNs show better robustness than the pint SNN baseline (the pink dashed line) in terms of both mCE and rmCE. On TinyImageNet-C, which poses a greater challenge than the previous task, networks with 4-branch DendSN and leaky ReLU branch nonlinearity yield much lower rmCEs compared to the point SNN, while the mCE is slightly higher given lower test accuracies on the clean test set. Reducing the number of branches leads to a decrease in mCE (due to better performance on the clean test set) and an increase in rmCE (due to worse performance on the corrupted test set). From these observations, we can conclude that more dendritic branches are advantageous for noise robustness in sacrifice of clean accuracy.

In the aforementioned cases, noises are added to static images. We next investigate DendSNNs' robustness against corruptions added to event streams. Similar to CIFAR-10-C and TinyImageNet-C, we construct a corrupted version of the DVS Gesture dataset [73] by applying multiple types of corruptive transformations with different levels of severities to the events in the test set, as demonstrated in Figure 55g. The transformation pipeline is further introduced in <8> and Table 56. Experimental procedures mirror those of CIFAR-10-C, and the resulting mCEs and rmCEs are demonstrated in Figure 55h. Five of the six DendSNNs obtain significantly lower rmCEs and comparable mCEs compared to the baseline. Notice that a network of DendSNs with 2 branches, leaky ReLU activation function and learnable branch strengths can even achieve a rmCE of <MATH> with a mCE of <MATH>, indicating remarkable robustness and comparable clean accuracy. Taken together, these results support the hypothesis that DendSNNs are more robust against noise than point SNNs.

Figure 7: DendSNNs' robustness against adversarial attack and few-shot learning performances. a) The procedures of the adversarial attack experiment. The left part depicts an ``attack-on-model" setup, where adversarial attacks are applied directly to the model of interest. The right part, however, portrays a ``black-box" scenario, where another shared baseline model is employed to generate the adversarial samples. b) DendSNNs' robustness against adversarial attack, tested on the FashionMNIST benchmark. Their performances are compared to a point SNN with similar architecture (the pink dashed line). Lower rmAE indicates better robustness. In the left plot, attacks are applied directly to the models of interest, while in the right plot, attacks are applied to a shared baseline model. c) The procedures of few-shot learning using Prototypical Network. In each episode, there is a support set with multiple categories, each including several samples. These samples are processed by a trained feature extractor to generate one prototype for each category. These prototypes are then employed to classify the samples in the query set, using the nearest-neighbor principle. d) DendSNNs' performances on the miniImageNet few-shot learning benchmark. The models are trained using either episodic training (top) or classical training (bottom) pipeline, and then evaluated using 5-way 1-shot, 5-way 5-shot, 10-way 1-shot, and 10-way 5-shot classification accuracies. DendSNNs obtain better results compared to point SNN.

Robustness against adversarial attacks

Besides noise, adversarial attacks represent another form of data corruption obtained by applying tiny yet intentionally worst-case perturbations to original samples [74]. Enhancing the robustness of neural networks against adversarial attacks is critical for security concerns. Previous studies have shown that deep SNNs are inherently more robust against adversarial attacks compared to non-spiking ANNs [75]. To check whether DendSNNs are less vulnerable to adversarial attacks than traditional SNNs, we conduct experiments based on the FashionMNIST dataset (the left part of Figure 58a). After training the models of interest on the original dataset, we employ the fast gradient sign method (FGSM) [74] to generate adversarial samples with respect to these models. The models' error rates on both the original and adversarial test sets under various perturbation amplitudes <MATH> are then recorded. Finally, we compute the relative mean adversarial error (rmAE), a metric similar to rmCE [77], as a summarized measure (refer to <9>). The left plot in Figure 58b compares rmAEs of different models. Remarkably, 10 out of 12 DendSNN configurations achieve lower rmAEs than the point SNN baseline, suggesting better robustness against adversarial attacks. Mexican hat dendritic nonlinearity can further enhance the models' resistance against attacks, and the network of DendSNs with 1 branch and parameter-free attentive branch strength yields the lowest rmAE of <MATH>.

In the previous setup, adversarial attacks are applied directly to the models of interest. As a result, the test set for one model is different from another, leading to unfairness. To make the comparisons more reasonable, we next adopt a black-box adversarial attack setting, as shown in the right part of Figure 60a. This time, adversarial test samples are generated with regard to a shared baseline model whose architecture is different from all models of interest. All the other settings are identical to the previous case. Despite the black-box adjustment, the final results demonstrated in the right part of Figure 60b exhibit a trend similar to the previous setting. In summary, DendSNNs are more robust against adversarial attacks than point SNNs.

Few-shot learning

Conventionally, supervised training of deep neural networks relies heavily on a substantial amount of labeled training data. Obtaining so much labeled data, however, is often costly and time-consuming. Hence, it's expected that machine learning models can effectively generalize to unseen domains and make accurate predictions when provided with very limited labeled training data. Here, we assess the few-shot learning capabilities of different backbone models using the miniImageNet benchmark [78]. We adopt the Prototypical Network paradigm [79] (Figure 62c), and choose SEW ResNet18 architecture [80] as the backbone for feature extraction due to its relatively high performance. Both episodic training [78] and classical training pipelines are constructed (see <10> for details), and the models are evaluated using 5-way 1-shot, 5-way 5-shot, 10-way 1-shot, and 10-way 5-shot classification accuracies. As shown in Figure 62d, all four types of accuracies under both episodic and classical training settings are elevated when a point SNN backbone is replaced with a DendSNN variant, regardless of the value of <MATH> and the configuration of dendritic branch strengths. These results indicate that multiple nonlinear dendritic branches can enhance the feature extractor's robustness to variations in the input domain. DendSNNs are thus capable of extracting more generalizable features and distance metrics [79]. This insight offers potential solutions to alleviate the data-hungry bottleneck in various machine learning applications.

