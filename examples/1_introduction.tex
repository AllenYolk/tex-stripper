\section*{Introduction}
\label{sec:introduction}
% comment ........

\begin{figure}[tp!]
    \centering
    \includegraphics[width=\textwidth]{img/final/introduction.drawio.pdf}
    \caption{
        \textbf{A comparison between deep neural networks with point neurons and biophysical neural network models with multi-compartment neurons.}
        \textbf{a)} Typically, deep neural networks (lower) adopt point neuron models (upper) as their basic computational units. Both the perceptron in deep ANNs and the LIF model in deep SNNs belong to this category. They simplify the neuron's morphology to a single spatial point.
        \textbf{b)} Multi-compartment models (upper) in neuroscience capture the detailed morphologies of real neurons and have complex dendritic dynamics. By connecting these intricate neurons, biophysical network models (lower) are established to simulate small-scale neural circuits in the brain. Notice that there may be multiple synaptic connections between a pair of neurons.
        \textbf{c)} Deep neural networks based on point neurons are computationally efficient and convenient to construct and train, but fall short on bio-plausibility and single-neuron expressivity (blue box). Biophysical networks with multi-compartment neurons have remarkable expressivity and can simulate the complex properties of real neurons, but cannot be easily scaled up to large networks (green box). There is a gap between these two types of models. In this work, we propose DendSN and DendSNN, combining dendritic computation with the design principles of deep learning to power up deep SNNs. We mainly focus on two questions: \textit{how} to ensure model flexibility and scalability while improving expressivity, and \textit{what} the effects of dendritic computation on deep neural networks are in diverse machine learning scenarios.
    }
    \label{fig:introduction}
\end{figure}

% this is a comment!!
The past decade has witnessed the success of deep learning across diverse domains, including computer vision \cite{krizhevsky2017alexnet,he2016resnet}, natural language processing~\cite{vaswani2017transformer,devlin2019bert,openai2023gpt4}, and autonomous driving \cite{lechner2020ncp,vorbach2021causal}. With the surge in the performance of parallel computing devices like GPUs, increasingly deeper artificial neural networks (ANNs) with meticulously crafted architectures can be efficiently trained and conveniently deployed for real-world applications~\cite{coates2013hpc,chetlur2014cudnn,jia2019dissecting}. In addition, inspired by the information processing mechanisms of biological neural circuits, spiking neural networks (SNNs) have emerged as a potentially more bio-plausible and energy-efficient alternative to ANNs \cite{hassabis2017neuroscience,roy2019towards}. Recent advances in neuromorphic hardware \cite{davies2018loihi,debole2019truenorth,pei2019tianjic} and SNN programming frameworks \cite{hazan2018bindsnet,eshraghian2023snntorch,fang2023spikingjelly} have further fueled the interest in deep SNNs, positioning them as promising models for the next generation of neural networks~\cite{maass1997networks}. 

The fundamental issue in modern SNN researches is to combine spiking neuronal dynamics with deep network architectures and further identify the most suitable application scenarios. Cutting-edge studies in this field are leveraging ANN building blocks like residual connections \cite{he2016resnet} and self-attention mechanisms \cite{vaswani2017transformer} for building up their spiking variants \cite{zheng2020going,fang2021sew,hu2023advancing,zhou2023spikformer,zhou2023spikingformer,yao2023spikedriventransformer,yao2024spikedriventransformer}, powering up deep SNNs and yielding enhanced performance. However, much less attention is directed towards the intricacies of the underlying neuron model. Previous studies in neuroscience have found that the computational expressivity of a single biological neuron rivals that of a multi-layer neural network consisting of thousands of artificial neurons \cite{poirazi2003pyramidal,jadi2014twolayer,tzilivaki2018dogma,beniaguev2021cortical}. This suggests an urgent need to rethink the design of individual neurons in deep neural networks, and biological neurons may serve as a valuable source of inspiration for possible enhancements. Nevertheless, how to integrate the remarkable expressivity of biological neurons into neurons of deep neural networks for improved performance in practical tasks remains to be explored. % this is another comment!!!

The dynamic mechanisms of biological neurons arise mainly from two sources: dendritic computations and somatic dynamics. Most existing deep SNNs, however, are built upon oversimplified neurons such as the leaky integrate-and-fire (LIF) model \cite{lapicque1907lif}, treating the entire neuron as a single spatial point and only depicting the dynamic properties of the soma. Models like the parametric leaky integrate-and-fire (PLIF) neuron \cite{fang2020plif} and the few-spikes neuron (FS-neuron) \cite{stockl2021optimized} attempt to enhance neuron-level dynamics by introducing learnable internal parameters, but still fail to take dendrites into account. These spiking neuron models, as well as their non-spiking counterparts in ANNs \cite{sanger1958perceptron}, are termed \textbf{point neuron models} due to their reduced morphology and dynamics (Figure \ref{fig:introduction}a). Notwithstanding their computational efficiency and reduced memory usage, these prevalent models sacrifice essential computational properties of dendrites, resulting in limited single-neuron expressivity. The oversimplification of point neuron models is a bottleneck that constrains the overall expressivity of deep SNNs and hinders the networks' ability to perform intricate computations for challenging tasks, as summarized within the blue box in Figure \ref{fig:introduction}c. 

In stark contrast, neuroscientists use interconnected \textbf{multi-compartment models} to simulate the biophysical activities of small-scale neuron circuits in the brain \cite{hines2001NEURON,stimberg2019Brian2}. As illustrated in Figure \ref{fig:introduction}b, multi-compartment models portray neuronal morphology at a fine granularity, and use differential equations with hundreds of variables to describe neuronal dynamics \cite{poirazi2003arithmetic,schutter1994purkinje,hay2011pyramidal}. They manage to capture critical computational properties present in biological dendrites \cite{london2005dendritic,payeur2019dendriticinfo,acharya2022dendritic}, including the passive attenuation of input signals \cite{mengual2020dendsoma}, the active generation of dendritic spikes \cite{major2013active}, as well as the selection and multiplexing of information \cite{payeur2019dendriticinfo}. As a result, multi-compartment models exhibit remarkable neuron-level expressivity (Figure \ref{fig:introduction}c, green box). Adding structural complexity and dendritic nonlinear dynamics to the neurons in deep SNNs is thus an intuitive and promising strategy for elevating the networks' expressivity and performance. The research community, however, has overlooked the role of dendritic computation in deep SNNs so far.

The idea of incorporating dendritic computation into deep learning, nonetheless, poses challenges related to network scalability and model flexibility. The success of deep learning models is primarily attributed to their large scales and intricate architectures. However, simulating large populations of multi-compartment neurons with complex dynamics is computationally prohibitive; even if these models can be simplified to reduce computational overhead, they are not trivially compatible with today's complex network architectures (see Figure \ref{fig:introduction}c, green box). For instance, some pioneering works focusing on bio-plausibility encounter scalability difficulties as network sizes increase due to the burden of solving differential equations for complicated neuronal dynamics \cite{guerguiev2017towards,sacramento2018microcircuits,payeur2021burst}. A recent study by Zheng et al. \cite{zheng2024temporal} simplifies multi-compartment models and proposes the DH-LIF neuron with temporal dendritic heterogeneity. They manage to build fully connected dendritic SNNs with recurrent connections and achieve competitive performances on tasks with rich timescales. Nevertheless, their neuron model has not been applied to networks with more than five layers due to high computational overhead, nor has it extended to prevalent SNN architectures like convolutional networks and Transformers. Some ANN neurons incorporate properties of dendritic computation \cite{wu2018improved,iyer2022catastrophe,georgescu2023apical} while neglecting temporal dynamics. Yet, they remain confined to small-scale fully connected networks. Thus, the fundamental question of \textit{how} to ensure model flexibility and scalability while improving neuron-level expressivity is still unresolved (Figure \ref{fig:introduction}c, upper part of the ring). We emphasize the necessity of adapting dendritic neurons to diverse network architectures; we also claim that two core requirements must be met to overcome the scalability issues: (1) developing a dendritic neuron model with highly parallelized simulation algorithms, and (2) leveraging the computational power of modern GPUs through low-level programming languages. Tackling these challenges is crucial and imperative for the practical applications of dendritic neural networks to complicated tasks.

Furthermore, while dendritic computation has been extensively studied \cite{london2005dendritic,hay2011pyramidal,payeur2019dendriticinfo,li2019dendritic,bicknell2021synaptic}, its influences on deep neural networks remain largely unexplored. Most previous works narrowly focus on one aspect of dendrites' benefits to deep learning, lacking a comprehensive study on the diverse network-level impact of dendrites in various machine learning scenarios \cite{guerguiev2017towards,payeur2021burst,wu2018improved,georgescu2023apical,zheng2024temporal}. As demonstrated in the middle of Figure \ref{fig:introduction}c, understanding the influence of dendritic computation on deep neural networks in diverse task settings is another critical issue that needs to be resolved.

To address these challenges, we propose the dendritic spiking neuron (DendSN) model that incorporates multiple dendritic branches with nonlinear dynamics. We demonstrate that DendSN has significantly higher expressivity than point spiking neurons, mainly due to its enhanced dendritic branch weighting mechanism. An acceleration algorithm is further proposed to parallelize dendritic dynamics computation over time, making the extra overhead negligible compared to point spiking neurons. We then introduce a universal solution to seamlessly integrate DendSNs into various deep SNN architectures, showcasing the flexibility of our model. Triton kernels \cite{tillet2019triton} are developed to fully exploit the power of GPUs, enabling dendritic spiking neural networks (DendSNNs) to reach the same depths as traditional deep SNN architectures while keeping affordable computational costs. DendSNNs are compatible with existing large-scale SNN training methods, and demonstrate improved accuracies compared to point SNNs in static image and event-based data classification tasks, even without introducing extra learnable parameters. We next comprehensively evaluate DendSNNs' performances across various settings to show their superior network-level expressivity and robustness compared to point SNNs. Inspired by dendritic modulation mechanism and synaptic clustering phenomenon observed in biological neural circuits \cite{wybo2023NMDA,cichon2015branch,limbacher2020cluster}, we propose dendritic branch gating (DBG), a novel algorithm designed for continual learning. By adjusting dendritic branch strengths based on task context signals, DBG effectively reduces interference between synaptic inputs from different tasks. Moreover, additional experiments reveal that DendSN can enhance the robustness of SNNs against noise and adversarial attacks, with improved performances also observed in few-shot learning scenarios. To the best of our knowledge, this is the first work that demonstrates the possibility and feasibility of constructing and training SNNs incorporating multiple nonlinear dendritic branches with depth and scale comparable to traditional deep SNNs, and conducts a systematic study on the benefits of dendritic computation for deep networks in diverse machine learning scenarios. 
